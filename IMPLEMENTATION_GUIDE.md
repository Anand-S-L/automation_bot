# State-of-the-Art RL Farming Agent - Complete Implementation Guide

**Status: Phase 2 Complete - Perception System Implemented**
**Timeline: 3-month full implementation**

## ğŸ¯ What We've Built

### âœ… Phase 1: Foundation (Complete)
- Deep Q-Network (DQN) architecture
- 15-action space (movement + combat + camera)
- Experience replay buffer
- Training infrastructure

### âœ… Phase 2: Perception System (Complete)
- **Health/Mana Detection** (`perception/health_detection.py`)
  - Color-based HP/mana bar detection
  - Low health/mana alerts
  - Critical health detection
  
- **Enemy Detection** (`perception/enemy_detection.py`)
  - HP bar detection above enemies
  - Target indicator recognition
  - Minimap enemy dots
  - Distance estimation
  
- **Reward Detection** (`perception/reward_detection.py`)
  - OCR for kill notifications
  - XP gain tracking
  - Loot detection
  - Damage numbers
  - Death detection
  - Performance metrics (kills/min, XP/min)

- **Enhanced RL Agent** (`enhanced_rl_agent.py`)
  - Dual-input neural network (visual + state vector)
  - Integrated perception systems
  - Rich reward calculation
  - TensorBoard support

## ğŸ“¦ Installation

### Step 1: Install PyTorch (CRITICAL)
```powershell
# For CUDA (NVIDIA GPU) - RECOMMENDED
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121

# For CPU only (slower training)
pip install torch torchvision
```

### Step 2: Install OCR Libraries
```powershell
# Install EasyOCR (better accuracy, GPU-accelerated)
pip install easyocr

# Install Tesseract OCR (faster, CPU)
# 1. Download from: https://github.com/UB-Mannheim/tesseract/wiki
# 2. Install to C:\Program Files\Tesseract-OCR\
# 3. Add to PATH
pip install pytesseract
```

### Step 3: Install All Dependencies
```powershell
pip install -r requirements_rl.txt
```

### Step 4: Verify Installation
```powershell
python -c "import torch; print('PyTorch:', torch.__version__, 'CUDA:', torch.cuda.is_available())"
python -c "import easyocr; print('EasyOCR: OK')"
python -c "import cv2; print('OpenCV:', cv2.__version__)"
```

## ğŸ§ª Testing Perception Modules

### Test Health Detection
```powershell
python perception/health_detection.py
```
**What to look for:**
- Green rectangles around HP/mana bars
- Percentage values displayed
- "Low HP" or "CRITICAL HP!" warnings

**Calibration:**
- Press 'c' to calibrate
- Manually edit `config_rl.json` to set bar regions

### Test Enemy Detection
```powershell
python perception/enemy_detection.py
```
**What to look for:**
- Red rectangles around enemies
- Yellow rectangles around targeted enemy
- Enemy count displayed
- Distance scores shown

### Test Reward Detection
```powershell
python perception/reward_detection.py
```
**What to look for:**
- Kill notifications detected
- XP bar changes tracked
- Statistics displayed (kills, XP, loot)
- Press 'r' to reset stats

## âš™ï¸ Configuration

Create `config_rl.json`:
```json
{
  "game_region": [0, 0, 1920, 1080],
  "minimap_region": [1670, 50, 200, 200],
  
  "health_detection": {
    "detection_mode": "color",
    "hp_bar_region": null,
    "mana_bar_region": null,
    "low_health_threshold": 30,
    "low_mana_threshold": 20
  },
  
  "enemy_detection": {
    "detection_method": "hybrid",
    "min_hp_bar_area": 50,
    "max_hp_bar_area": 5000
  },
  
  "reward_detection": {
    "use_ocr": true,
    "ocr_engine": "easyocr",
    "notification_region": [600, 100, 720, 200],
    "xp_bar_region": [400, 950, 1120, 30]
  },
  
  "learning_rate": 0.0001,
  "memory_size": 10000,
  "gamma": 0.99,
  "epsilon_start": 1.0,
  "epsilon_end": 0.01,
  "epsilon_decay": 0.995,
  "batch_size": 32
}
```

## ğŸš€ Training Workflow

### Phase 2B: Initial Training (This Week)
```powershell
# Run original training script to test integration
python rl_farming_agent.py
# Choose option 1 (Train)
```

**Expected behavior:**
- Agent captures enhanced game state
- Detects HP, enemies, rewards
- Learns from actual game events
- Rewards based on real performance

**Training parameters:**
- Episodes: Start with 100-500
- Each episode: Until death or timeout (5 minutes)
- Training time: 2-6 hours depending on GPU

### Monitor Training
```powershell
# In separate terminal
tensorboard --logdir=runs
# Open http://localhost:6006
```

## ğŸ“Š What the Agent Learns

### Reward Structure (Enhanced)
```
+10.0  - Kill enemy
+5.0   - Loot common item
+10.0  - Loot rare item
+20.0  - Loot epic item
+0.1   - Per XP point gained
+0.1   - Per damage dealt
-0.5   - Per damage taken
-50.0  - Death
+25.0  - Level up
+0.01  - Moving toward enemy
+0.05  - Surviving with low HP
+0.02  - Engaging in combat
-0.01  - Step penalty (efficiency)
```

### Learning Progression
**Episodes 1-100:** Random exploration
- High epsilon (0.9-0.5)
- Learning to navigate
- Discovering rewards

**Episodes 100-300:** Basic combat
- Epsilon 0.5-0.2
- Engaging enemies
- Using attack action
- Avoiding death

**Episodes 300-500:** Efficient farming
- Epsilon 0.2-0.05
- Optimal attack patterns
- Loot collection
- Health management

**Episodes 500+:** Mastery
- Epsilon 0.05-0.01
- Consistent kills
- Minimal deaths
- Efficient routing

## ğŸ” Debugging

### Common Issues

**1. "Import cv2 could not be resolved"**
```powershell
pip install opencv-python
```

**2. "Import torch could not be resolved"**
```powershell
pip install torch torchvision
```

**3. "EasyOCR not working"**
- Requires CUDA for GPU acceleration
- Falls back to Tesseract automatically
- Check: `python -c "import easyocr; print('OK')"`

**4. "Health/Enemy not detected"**
- Game UI may be different
- Run test scripts to visualize
- Adjust color ranges in detector config
- Manually set regions in config file

**5. "Agent just moves randomly"**
- Normal in early episodes (exploration)
- Check epsilon decay
- Verify rewards are being calculated
- Check TensorBoard for learning curves

### Logging
All modules print debug info:
```
[HealthDetector] HP detection error: ...
[EnemyDetector] Minimap detection error: ...
[RewardDetector] OCR error: ...
```

Enable verbose mode by editing detector `__init__`:
```python
self.verbose = True  # Add to any detector
```

## ğŸ“ˆ Performance Expectations

### Training Time (RTX 3060 GPU)
- 100 episodes: ~30 minutes
- 500 episodes: ~2.5 hours
- 1000 episodes: ~5 hours

### CPU Training (i7 or better)
- 100 episodes: ~1.5 hours
- 500 episodes: ~8 hours
- 1000 episodes: ~16 hours

### Memory Usage
- RAM: 4-8 GB
- VRAM (GPU): 2-4 GB
- Disk: 100-500 MB for models

### Performance Metrics (After 500 episodes)
- Win rate: 60-80%
- Kills per episode: 10-30
- Deaths per episode: 0.2-0.5
- Average reward: 50-150

## ğŸ® Using Trained Agent

```powershell
python rl_farming_agent.py
# Choose option 3 (Play/Farm)
```

**What happens:**
- Agent loads trained model
- Captures game state continuously
- Selects actions based on policy (epsilon=0.01, mostly greedy)
- Farms automatically
- Press ESC to stop

## ğŸ› ï¸ Next Development Phases

### Phase 3: Advanced Reward System (Week 3-4)
- Fine-tune reward values
- Add shaped rewards for combos
- Implement multi-kill bonuses
- Add efficiency metrics

### Phase 4: Enhanced Neural Architecture (Week 4-5)
- Dueling DQN
- LSTM for temporal patterns
- Attention mechanisms
- Better feature extraction

### Phase 5: Advanced RL Algorithms (Week 5-6)
- Double DQN
- Prioritized experience replay
- Noisy networks for exploration
- Rainbow DQN (future)

### Phase 6: Skills & Combat System (Week 6-7)
- Skill detection (OCR for cooldowns)
- Combo learning
- Resource management
- Advanced positioning

### Phase 7: Navigation & Strategy (Week 7-8)
- Farming route optimization
- Multi-area support
- Hierarchical RL
- Adaptive strategies

### Phase 8: Safety & Robustness (Week 8-9)
- Death recovery
- Inventory management
- Error handling
- Anti-detection measures

### Phase 9: Multi-Agent (Week 9-10)
- Parallel training
- Transfer learning
- Meta-learning
- Distributed training

### Phase 10: Production (Week 10-12)
- Web dashboard
- Cloud training
- Model versioning
- Complete documentation

## ğŸ“ Support & Resources

### Documentation Files
- `RL_GUIDE.md` - Original RL guide
- `RL_ROADMAP.py` - Detailed 12-week plan
- This file - Implementation guide

### Test Scripts
- `perception/health_detection.py` - Test HP/mana
- `perception/enemy_detection.py` - Test enemy detection
- `perception/reward_detection.py` - Test OCR rewards

### Training Scripts
- `rl_farming_agent.py` - Original agent
- `enhanced_rl_agent.py` - Phase 2 integrated agent

### Useful Commands
```powershell
# Check GPU usage
nvidia-smi

# Monitor training
tensorboard --logdir=runs

# Kill all Python processes (emergency stop)
taskkill /F /IM python.exe
```

## ğŸ¯ Current Status

âœ… **Complete:**
- Foundation (DQN, actions, training loop)
- Health/mana detection
- Enemy detection
- Reward detection (OCR)
- Enhanced agent integration
- TensorBoard support

âš ï¸ **In Progress:**
- Initial training and testing
- Calibration for Evil Lands
- Hyperparameter tuning

âŒ **Not Started:**
- Skill system
- Advanced algorithms (Double DQN, PER)
- Hierarchical navigation
- Production UI

## ğŸ’¡ Tips for Success

1. **Start small**: Train 100 episodes first
2. **Test perception**: Verify all detectors work
3. **Monitor TensorBoard**: Watch reward curves
4. **Save often**: Model checkpoints every 100 episodes
5. **Iterate**: Adjust rewards based on behavior
6. **Be patient**: Learning takes hours/days
7. **Use GPU**: 5-10x faster training

## ğŸ† Success Criteria

**Phase 2 Complete When:**
- âœ… All perception modules working
- âœ… Agent detects HP, enemies, rewards
- âœ… Rewards calculated from game events
- â³ Agent trains for 500+ episodes
- â³ Win rate > 60%
- â³ Consistent kill performance

**Ready for Phase 3 When:**
- Agent reliably kills enemies
- Death rate < 0.5 per episode
- Loot collection working
- No major bugs

---

**Built with â¤ï¸ for state-of-the-art AI gaming**

*Version: 2.0 - Phase 2 Complete*
*Last Updated: November 2025*
